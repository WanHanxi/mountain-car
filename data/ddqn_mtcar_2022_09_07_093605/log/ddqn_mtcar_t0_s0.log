[2022-09-07 14:41:42,270 PID:8853 INFO openai.py __init__] OpenAIEnv:
- env_spec = {'max_frame': 1000000, 'max_t': None, 'name': 'MountainCar-v0'}
- eval_frequency = 500
- log_frequency = 500
- frame_op = None
- frame_op_len = None
- image_downsize = (84, 84)
- normalize_state = False
- reward_scale = None
- num_envs = 1
- name = MountainCar-v0
- max_t = 200
- max_frame = 1000000
- to_render = True
- is_venv = False
- clock_speed = 1
- clock = <slm_lab.env.base.Clock object at 0x7f6f0432f3c8>
- done = False
- total_reward = nan
- u_env = <TrackReward<TimeLimit<MountainCarEnv<MountainCar-v0>>>>
- observation_space = Box(2,)
- action_space = Discrete(3)
- observable_dim = {'state': 2}
- action_dim = 3
- is_discrete = True
[2022-09-07 14:41:43,531 PID:8853 INFO net_util.py load_algorithm] Loading algorithm DoubleDQN nets ['net', 'target_net'] from data/ddqn_mtcar_2022_09_07_093605/model/ddqn_mtcar_t0_s0_ckpt-best_*.pt
[2022-09-07 15:21:28,705 PID:16337 INFO openai.py __init__] OpenAIEnv:
- env_spec = {'max_frame': 1000000, 'max_t': None, 'name': 'MountainCar-v0'}
- eval_frequency = 500
- log_frequency = 500
- frame_op = None
- frame_op_len = None
- image_downsize = (84, 84)
- normalize_state = False
- reward_scale = None
- num_envs = 1
- name = MountainCar-v0
- max_t = 200
- max_frame = 1000000
- to_render = True
- is_venv = False
- clock_speed = 1
- clock = <slm_lab.env.base.Clock object at 0x7f089242b3c8>
- done = False
- total_reward = nan
- u_env = <TrackReward<TimeLimit<MountainCarEnv<MountainCar-v0>>>>
- observation_space = Box(2,)
- action_space = Discrete(3)
- observable_dim = {'state': 2}
- action_dim = 3
- is_discrete = True
[2022-09-07 15:21:29,663 PID:16337 INFO net_util.py load_algorithm] Loading algorithm DoubleDQN nets ['net', 'target_net'] from data/ddqn_mtcar_2022_09_07_093605/model/ddqn_mtcar_t0_s0_ckpt-best_*.pt
[2022-09-07 15:21:29,669 PID:16337 INFO base.py end_init_nets] Loaded algorithm models for lab_mode: enjoy
[2022-09-07 15:21:29,674 PID:16337 INFO base.py __init__] DoubleDQN:
- agent = <slm_lab.agent.Agent object at 0x7f07f00a0198>
- action_pdtype = Argmax
- action_policy = <function epsilon_greedy at 0x7f07f6c4abf8>
- explore_var_spec = {'end_step': 10000,
 'end_val': 0.2,
 'name': 'linear_decay',
 'start_step': 1000,
 'start_val': 1.0}
- training_start_step = 32
- gamma = 0.99
- training_batch_iter = 8
- training_iter = 4
- training_frequency = 4
- to_train = 0
- explore_var_scheduler = <slm_lab.agent.algorithm.policy_util.VarScheduler object at 0x7f07f00b7400>
- net = MLPNet(
  (model): Sequential(
    (0): Linear(in_features=2, out_features=32, bias=True)
    (1): SELU()
    (2): Linear(in_features=32, out_features=32, bias=True)
    (3): SELU()
  )
  (model_tail): Sequential(
    (0): Linear(in_features=32, out_features=3, bias=True)
  )
  (loss_fn): MSELoss()
)
- target_net = MLPNet(
  (model): Sequential(
    (0): Linear(in_features=2, out_features=32, bias=True)
    (1): SELU()
    (2): Linear(in_features=32, out_features=32, bias=True)
    (3): SELU()
  )
  (model_tail): Sequential(
    (0): Linear(in_features=32, out_features=3, bias=True)
  )
  (loss_fn): MSELoss()
)
- net_names = ['net', 'target_net']
- optim = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    initial_lr: 0.02
    lr: 0.0001145283379404471
    maximize: False
    weight_decay: 0
)
- lr_scheduler = <torch.optim.lr_scheduler.StepLR object at 0x7f07f00b7a90>
- global_net = None
- global_target_net = None
- online_net = MLPNet(
  (model): Sequential(
    (0): Linear(in_features=2, out_features=32, bias=True)
    (1): SELU()
    (2): Linear(in_features=32, out_features=32, bias=True)
    (3): SELU()
  )
  (model_tail): Sequential(
    (0): Linear(in_features=32, out_features=3, bias=True)
  )
  (loss_fn): MSELoss()
)
- eval_net = MLPNet(
  (model): Sequential(
    (0): Linear(in_features=2, out_features=32, bias=True)
    (1): SELU()
    (2): Linear(in_features=32, out_features=32, bias=True)
    (3): SELU()
  )
  (model_tail): Sequential(
    (0): Linear(in_features=32, out_features=3, bias=True)
  )
  (loss_fn): MSELoss()
)
[2022-09-07 15:21:29,675 PID:16337 INFO __init__.py __init__] Agent:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 500,
 'experiment': 0,
 'experiment_ts': '2022_09_07_093605',
 'git_sha': '88006fcd2910f6fefbcb7c0970daa1d5b6135cd0',
 'graph_prepath': 'data/ddqn_mtcar_2022_09_07_093605/graph/ddqn_mtcar_t0_s0',
 'info_prepath': 'data/ddqn_mtcar_2022_09_07_093605/info/ddqn_mtcar_t0_s0',
 'log_frequency': 500,
 'log_prepath': 'data/ddqn_mtcar_2022_09_07_093605/log/ddqn_mtcar_t0_s0',
 'max_session': 1,
 'max_trial': 5,
 'model_prepath': 'data/ddqn_mtcar_2022_09_07_093605/model/ddqn_mtcar_t0_s0',
 'prepath': 'data/ddqn_mtcar_2022_09_07_093605/ddqn_mtcar_t0_s0',
 'random_seed': 1662578488,
 'resume': False,
 'rigorous_eval': 0,
 'session': 0,
 'trial': 0}
- agent_spec = {'algorithm': {'action_pdtype': 'Argmax',
               'action_policy': 'epsilon_greedy',
               'explore_var_spec': {'end_step': 10000,
                                    'end_val': 0.2,
                                    'name': 'linear_decay',
                                    'start_step': 1000,
                                    'start_val': 1.0},
               'gamma': 0.99,
               'name': 'DoubleDQN',
               'training_batch_iter': 8,
               'training_frequency': 4,
               'training_iter': 4,
               'training_start_step': 32},
 'memory': {'alpha': 0.6000000000000001,
            'batch_size': 32,
            'epsilon': 0.001,
            'max_size': 10000,
            'name': 'PrioritizedReplay',
            'use_cer': False},
 'name': 'DoubleDQN',
 'net': {'clip_grad_val': 0.5,
         'cuda_id': 0,
         'gpu': True,
         'hid_layers': [32, 32],
         'hid_layers_activation': 'selu',
         'loss_spec': {'name': 'MSELoss'},
         'lr_scheduler_spec': {'gamma': 0.9,
                               'name': 'StepLR',
                               'step_size': 1000},
         'optim_spec': {'lr': 0.02, 'name': 'Adam'},
         'polyak_coef': 0.1,
         'type': 'MLPNet',
         'update_frequency': 32,
         'update_type': 'polyak'}}
- name = DoubleDQN
- body = body: {
  "agent": "<slm_lab.agent.Agent object at 0x7f07f00a0198>",
  "env": "<slm_lab.env.openai.OpenAIEnv object at 0x7f07f6dc9160>",
  "a": 0,
  "e": 0,
  "b": 0,
  "aeb": "(0, 0, 0)",
  "explore_var": 0.2,
  "entropy_coef": NaN,
  "loss": NaN,
  "mean_entropy": NaN,
  "mean_grad_norm": NaN,
  "best_total_reward_ma": -Infinity,
  "total_reward_ma": NaN,
  "train_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "eval_df": "Empty DataFrame\nColumns: [epi, t, wall_t, opt_step, frame, fps, total_reward, total_reward_ma, loss, lr, explore_var, entropy_coef, entropy, grad_norm]\nIndex: []",
  "observation_space": "Box(2,)",
  "action_space": "Discrete(3)",
  "observable_dim": {
    "state": 2
  },
  "state_dim": 2,
  "action_dim": 3,
  "is_discrete": true,
  "action_type": "discrete",
  "action_pdtype": "Argmax",
  "ActionPD": "<class 'slm_lab.lib.distribution.Argmax'>",
  "memory": "<slm_lab.agent.memory.prioritized.PrioritizedReplay object at 0x7f07f0097fd0>"
}
- algorithm = <slm_lab.agent.algorithm.dqn.DoubleDQN object at 0x7f07f00b7470>
[2022-09-07 15:21:29,676 PID:16337 INFO logger.py info] Session:
- spec = {'cuda_offset': 0,
 'distributed': False,
 'eval_frequency': 500,
 'experiment': 0,
 'experiment_ts': '2022_09_07_093605',
 'git_sha': '88006fcd2910f6fefbcb7c0970daa1d5b6135cd0',
 'graph_prepath': 'data/ddqn_mtcar_2022_09_07_093605/graph/ddqn_mtcar_t0_s0',
 'info_prepath': 'data/ddqn_mtcar_2022_09_07_093605/info/ddqn_mtcar_t0_s0',
 'log_frequency': 500,
 'log_prepath': 'data/ddqn_mtcar_2022_09_07_093605/log/ddqn_mtcar_t0_s0',
 'max_session': 1,
 'max_trial': 5,
 'model_prepath': 'data/ddqn_mtcar_2022_09_07_093605/model/ddqn_mtcar_t0_s0',
 'prepath': 'data/ddqn_mtcar_2022_09_07_093605/ddqn_mtcar_t0_s0',
 'random_seed': 1662578488,
 'resume': False,
 'rigorous_eval': 0,
 'session': 0,
 'trial': 0}
- index = 0
- agent = <slm_lab.agent.Agent object at 0x7f07f00a0198>
- env = <slm_lab.env.openai.OpenAIEnv object at 0x7f07f6dc9160>
- eval_env = <slm_lab.env.openai.OpenAIEnv object at 0x7f07f6dc9160>
[2022-09-07 15:21:29,676 PID:16337 INFO logger.py info] Running RL loop for trial 0 session 0
[2022-09-07 15:21:30,149 PID:16337 INFO __init__.py log_summary] Trial 0 session 0 ddqn_mtcar_t0_s0 [train_df] epi: 0  t: 0  wall_t: 0  opt_step: 0  frame: 0  fps: 0  total_reward: nan  total_reward_ma: nan  loss: nan  lr: 0.000114528  explore_var: 0.2  entropy_coef: nan  entropy: nan  grad_norm: nan
[2022-09-07 15:21:34,131 PID:16337 INFO __init__.py log_summary] Trial 0 session 0 ddqn_mtcar_t0_s0 [train_df] epi: 3  t: 70  wall_t: 5  opt_step: 0  frame: 500  fps: 100  total_reward: -146  total_reward_ma: -146  loss: nan  lr: 0.000114528  explore_var: 0.2  entropy_coef: nan  entropy: nan  grad_norm: nan
[2022-09-07 15:21:37,628 PID:16337 INFO __init__.py log_summary] Trial 0 session 0 ddqn_mtcar_t0_s0 [train_df] epi: 7  t: 4  wall_t: 8  opt_step: 0  frame: 1000  fps: 125  total_reward: -135  total_reward_ma: -140.5  loss: nan  lr: 0.000114528  explore_var: 0.2  entropy_coef: nan  entropy: nan  grad_norm: nan
[2022-09-07 15:21:37,641 PID:16337 INFO __init__.py log_metrics] Trial 0 session 0 ddqn_mtcar_t0_s0 [train_df metrics] final_return_ma: -140.5  strength: 59.5  max_strength: 65  final_strength: 65  sample_efficiency: 0.00145378  training_efficiency: 0  stability: 1
[2022-09-07 15:21:41,126 PID:16337 INFO __init__.py log_summary] Trial 0 session 0 ddqn_mtcar_t0_s0 [train_df] epi: 10  t: 88  wall_t: 12  opt_step: 0  frame: 1500  fps: 125  total_reward: -144  total_reward_ma: -141.667  loss: nan  lr: 0.000114528  explore_var: 0.2  entropy_coef: nan  entropy: nan  grad_norm: nan
[2022-09-07 15:21:41,138 PID:16337 INFO __init__.py log_metrics] Trial 0 session 0 ddqn_mtcar_t0_s0 [train_df metrics] final_return_ma: -141.667  strength: 58.3333  max_strength: 65  final_strength: 56  sample_efficiency: 0.0012019  training_efficiency: 0  stability: 0.92437
